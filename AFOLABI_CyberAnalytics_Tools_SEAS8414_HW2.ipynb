{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kolawole-a2/Kola_Projects/blob/main/AFOLABI_CyberAnalytics_Tools_SEAS8414_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# --- Load the CSV ---\n",
        "def load_data(csv_path=None):\n",
        "    # For manual testing (you can use file upload or a direct path)\n",
        "    if csv_path:\n",
        "        df = pd.read_csv(csv_path, parse_dates=['Timestamp'])\n",
        "    else:\n",
        "        # Simulated CSV as fallback\n",
        "        from io import StringIO\n",
        "        sample_data = \"\"\"IP,Timestamp,Status,User Agent,DNS Query,Device Fingerprint,Session ID\n",
        "203.0.113.1,2025-06-15 21:00,FAIL,Chrome/120.0,api.example.com,df_hash_1234,sess_001\n",
        "203.0.113.1,2025-06-15 21:01,FAIL,Firefox/115.0,login.example.com,df_hash_1234,sess_002\n",
        "203.0.113.2,2025-06-15 21:02,SUCCESS,Chrome/120.0,app.example.com,df_hash_5678,sess_003\n",
        "203.0.113.3,2025-06-15 21:03,FAIL,Edge/121.0,api.example.com,df_hash_9012,sess_004\n",
        "203.0.113.3,2025-06-15 21:04,FAIL,Safari/17.0,login.example.com,df_hash_9012,sess_005\n",
        "203.0.113.4,2025-06-15 21:05,FAIL,Chrome/120.0,api.example.com,df_hash_3456,sess_006\n",
        "203.0.113.4,2025-06-15 21:06,FAIL,Firefox/115.0,login.example.com,df_hash_3456,sess_007\n",
        "203.0.113.5,2025-06-15 21:07,SUCCESS,Edge/121.0,app.example.com,df_hash_7890,sess_008\n",
        "203.0.113.6,2025-06-15 21:08,FAIL,Safari/17.0,api.example.com,df_hash_2345,sess_009\n",
        "203.0.113.6,2025-06-15 21:09,FAIL,Chrome/120.0,login.example.com,df_hash_2345,sess_010\n",
        "203.0.113.7,2025-06-15 21:10,FAIL,Firefox/115.0,api.example.com,df_hash_6789,sess_011\n",
        "203.0.113.7,2025-06-15 21:11,FAIL,Edge/121.0,login.example.com,df_hash_6789,sess_012\n",
        "203.0.113.8,2025-06-15 21:12,SUCCESS,Safari/17.0,app.example.com,df_hash_0123,sess_013\n",
        "203.0.113.9,2025-06-15 21:13,FAIL,Chrome/120.0,api.example.com,df_hash_4567,sess_014\n",
        "203.0.113.9,2025-06-15 21:14,FAIL,Firefox/115.0,login.example.com,df_hash_4567,sess_015\n",
        "203.0.113.10,2025-06-15 21:15,FAIL,Edge/121.0,api.example.com,df_hash_8901,sess_016\n",
        "203.0.113.10,2025-06-15 21:16,FAIL,Safari/17.0,login.example.com,df_hash_8901,sess_017\n",
        "203.0.113.1,2025-06-15 21:17,FAIL,Edge/121.0,api.example.com,df_hash_1234,sess_018\n",
        "203.0.113.3,2025-06-15 21:18,FAIL,Chrome/120.0,login.example.com,df_hash_9012,sess_019\n",
        "203.0.113.6,2025-06-15 21:19,FAIL,Firefox/115.0,api.example.com,df_hash_2345,sess_020\n",
        "\"\"\"\n",
        "        df = pd.read_csv(StringIO(sample_data), parse_dates=['Timestamp'])\n",
        "    return df\n",
        "\n",
        "# --- Compute Metrics ---\n",
        "def failure_rate(group):\n",
        "    total = len(group)\n",
        "    fails = group['Status'].str.upper().eq(\"FAIL\").sum()\n",
        "    return round((fails / total) * 100, 2)\n",
        "\n",
        "def user_agent_entropy(group):\n",
        "    user_agents = group['User Agent']\n",
        "    counts = Counter(user_agents)\n",
        "    probs = [count / len(user_agents) for count in counts.values()]\n",
        "    return round(entropy(probs, base=2), 2)\n",
        "\n",
        "def unique_dns_queries(group):\n",
        "    return group['DNS Query'].nunique()\n",
        "\n",
        "# --- Generate Report ---\n",
        "def generate_report(df):\n",
        "    grouped = df.groupby('IP').apply(lambda group: pd.Series({\n",
        "        'Failure Rate (%)': failure_rate(group),\n",
        "        'User Agent Count': group['User Agent'].nunique(),\n",
        "        'User Agent Entropy': user_agent_entropy(group),\n",
        "        'Unique DNS Queries': unique_dns_queries(group)\n",
        "    })).reset_index()\n",
        "\n",
        "    return grouped.sort_values('Failure Rate (%)', ascending=False)\n",
        "\n",
        "# --- Main Function ---\n",
        "def run_analysis(csv_path=None):\n",
        "    df = load_data(csv_path)\n",
        "    print(f\"Total Records Loaded: {len(df)}\")\n",
        "    report = generate_report(df)\n",
        "    print(\"\\n=== Descriptive Analytics Report by IP ===\\n\")\n",
        "    print(report.to_string(index=False))\n",
        "\n",
        "# Run directly (comment if used as module)\n",
        "if __name__ == \"__main__\":\n",
        "    run_analysis()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mfGGnBwMjGX",
        "outputId": "d55648a1-b2d6-4980-a6da-782bb2ae73c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Records Loaded: 20\n",
            "\n",
            "=== Descriptive Analytics Report by IP ===\n",
            "\n",
            "          IP  Failure Rate (%)  User Agent Count  User Agent Entropy  Unique DNS Queries\n",
            " 203.0.113.1             100.0               3.0                1.58                 2.0\n",
            "203.0.113.10             100.0               2.0                1.00                 2.0\n",
            " 203.0.113.3             100.0               3.0                1.58                 2.0\n",
            " 203.0.113.4             100.0               2.0                1.00                 2.0\n",
            " 203.0.113.7             100.0               2.0                1.00                 2.0\n",
            " 203.0.113.6             100.0               3.0                1.58                 2.0\n",
            " 203.0.113.9             100.0               2.0                1.00                 2.0\n",
            " 203.0.113.2               0.0               1.0                0.00                 1.0\n",
            " 203.0.113.5               0.0               1.0                0.00                 1.0\n",
            " 203.0.113.8               0.0               1.0                0.00                 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2-2147828834.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  grouped = df.groupby('IP').apply(lambda group: pd.Series({\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: Key Limitation in Descriptive Analytics\n",
        "Which limitation of the descriptive analytics approach most significantly undermines its effectiveness against these adaptive attackers?\n",
        "\n",
        "A. User Agent Entropy fails to detect spoofing\n",
        "B. Static IP grouping overlooks low-volume distributed attacks\n",
        "C. DNS Query Uniqueness fails against DGAs\n",
        "D. Failure Rate ignores CAPTCHA bypass success\n",
        "E. No behavioral anomaly detection\n",
        "F. Timestamp aggregation ignores pacing\n",
        "G. Lack of graph-based rate limiting"
      ],
      "metadata": {
        "id": "RbzKFrdmM4Mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "# Embedded dataset (20 rows from the case study)\n",
        "data = \"\"\"\n",
        "IP,Timestamp,Status,User Agent,DNS Query,Device Fingerprint,Session ID\n",
        "203.0.113.1,2025-06-15 21:00,FAIL,Chrome/120.0,api.example.com,df_hash_1234,sess_001\n",
        "203.0.113.1,2025-06-15 21:01,FAIL,Firefox/115.0,login.example.com,df_hash_1234,sess_002\n",
        "203.0.113.2,2025-06-15 21:02,SUCCESS,Chrome/120.0,app.example.com,df_hash_5678,sess_003\n",
        "203.0.113.3,2025-06-15 21:03,FAIL,Edge/121.0,api.example.com,df_hash_9012,sess_004\n",
        "203.0.113.3,2025-06-15 21:04,FAIL,Safari/17.0,login.example.com,df_hash_9012,sess_005\n",
        "203.0.113.4,2025-06-15 21:05,FAIL,Chrome/120.0,api.example.com,df_hash_3456,sess_006\n",
        "203.0.113.4,2025-06-15 21:06,FAIL,Firefox/115.0,login.example.com,df_hash_3456,sess_007\n",
        "203.0.113.5,2025-06-15 21:07,SUCCESS,Edge/121.0,app.example.com,df_hash_7890,sess_008\n",
        "203.0.113.6,2025-06-15 21:08,FAIL,Safari/17.0,api.example.com,df_hash_2345,sess_009\n",
        "203.0.113.6,2025-06-15 21:09,FAIL,Chrome/120.0,login.example.com,df_hash_2345,sess_010\n",
        "203.0.113.7,2025-06-15 21:10,FAIL,Firefox/115.0,api.example.com,df_hash_6789,sess_011\n",
        "203.0.113.7,2025-06-15 21:11,FAIL,Edge/121.0,login.example.com,df_hash_6789,sess_012\n",
        "203.0.113.8,2025-06-15 21:12,SUCCESS,Safari/17.0,app.example.com,df_hash_0123,sess_013\n",
        "203.0.113.9,2025-06-15 21:13,FAIL,Chrome/120.0,api.example.com,df_hash_4567,sess_014\n",
        "203.0.113.9,2025-06-15 21:14,FAIL,Firefox/115.0,login.example.com,df_hash_4567,sess_015\n",
        "203.0.113.10,2025-06-15 21:15,FAIL,Edge/121.0,api.example.com,df_hash_8901,sess_016\n",
        "203.0.113.10,2025-06-15 21:16,FAIL,Safari/17.0,login.example.com,df_hash_8901,sess_017\n",
        "203.0.113.1,2025-06-15 21:17,FAIL,Edge/121.0,api.example.com,df_hash_1234,sess_018\n",
        "203.0.113.3,2025-06-15 21:18,FAIL,Chrome/120.0,login.example.com,df_hash_9012,sess_019\n",
        "203.0.113.6,2025-06-15 21:19,FAIL,Firefox/115.0,api.example.com,df_hash_2345,sess_020\n",
        "\"\"\"\n",
        "\n",
        "# Load into DataFrame\n",
        "df = pd.read_csv(StringIO(data), parse_dates=[\"Timestamp\"])\n",
        "\n",
        "# Count attempts per IP\n",
        "attempt_counts = df['IP'].value_counts().reset_index()\n",
        "attempt_counts.columns = ['IP', 'Login Attempts']\n",
        "\n",
        "# Filter IPs with 3 or fewer attempts\n",
        "low_volume_ips = attempt_counts[attempt_counts['Login Attempts'] <= 3]\n",
        "\n",
        "print(\"=== Low-Volume IPs Used in Credential Stuffing (‚â§ 3 attempts) ===\")\n",
        "print(low_volume_ips.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuyjA54ANogt",
        "outputId": "9d8ff1fb-093e-4525-fc87-4edad94cf36f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Low-Volume IPs Used in Credential Stuffing (‚â§ 3 attempts) ===\n",
            "          IP  Login Attempts\n",
            " 203.0.113.1               3\n",
            " 203.0.113.3               3\n",
            " 203.0.113.6               3\n",
            "203.0.113.10               2\n",
            " 203.0.113.4               2\n",
            " 203.0.113.7               2\n",
            " 203.0.113.9               2\n",
            " 203.0.113.2               1\n",
            " 203.0.113.5               1\n",
            " 203.0.113.8               1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "****************************************************************************************************************************"
      ],
      "metadata": {
        "id": "-H_6rkEkXqFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1\n",
        "\"(case 1) Which traditional analytic is most undermined by attackers employing IP rotation in a credential stuffing attack, rendering it nearly useless?\"\n",
        "\t\tFailure Rate per IP\n",
        "\t\tStatic IP Grouping\n",
        "\t\tBasic Rate Limiting\n",
        "\t\tDNS Query Uniqueness\n",
        "\t\tUser Agent Entropy\n",
        "\t\tLouvain Clustering\n"
      ],
      "metadata": {
        "id": "-9HuHkT2UhEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "# === Load Scenario-1 Dataset ===\n",
        "data = \"\"\"\n",
        "IP,Timestamp,Status,User Agent,DNS Query,Device Fingerprint,Session ID\n",
        "203.0.113.1,2025-06-15 21:00,FAIL,Chrome/120.0,api.example.com,df_hash_1234,sess_001\n",
        "203.0.113.1,2025-06-15 21:01,FAIL,Firefox/115.0,login.example.com,df_hash_1234,sess_002\n",
        "203.0.113.2,2025-06-15 21:02,SUCCESS,Chrome/120.0,app.example.com,df_hash_5678,sess_003\n",
        "203.0.113.3,2025-06-15 21:03,FAIL,Edge/121.0,api.example.com,df_hash_9012,sess_004\n",
        "203.0.113.3,2025-06-15 21:04,FAIL,Safari/17.0,login.example.com,df_hash_9012,sess_005\n",
        "203.0.113.4,2025-06-15 21:05,FAIL,Chrome/120.0,api.example.com,df_hash_3456,sess_006\n",
        "203.0.113.4,2025-06-15 21:06,FAIL,Firefox/115.0,login.example.com,df_hash_3456,sess_007\n",
        "203.0.113.5,2025-06-15 21:07,SUCCESS,Edge/121.0,app.example.com,df_hash_7890,sess_008\n",
        "203.0.113.6,2025-06-15 21:08,FAIL,Safari/17.0,api.example.com,df_hash_2345,sess_009\n",
        "203.0.113.6,2025-06-15 21:09,FAIL,Chrome/120.0,login.example.com,df_hash_2345,sess_010\n",
        "203.0.113.7,2025-06-15 21:10,FAIL,Firefox/115.0,api.example.com,df_hash_6789,sess_011\n",
        "203.0.113.7,2025-06-15 21:11,FAIL,Edge/121.0,login.example.com,df_hash_6789,sess_012\n",
        "203.0.113.8,2025-06-15 21:12,SUCCESS,Safari/17.0,app.example.com,df_hash_0123,sess_013\n",
        "203.0.113.9,2025-06-15 21:13,FAIL,Chrome/120.0,api.example.com,df_hash_4567,sess_014\n",
        "203.0.113.9,2025-06-15 21:14,FAIL,Firefox/115.0,login.example.com,df_hash_4567,sess_015\n",
        "203.0.113.10,2025-06-15 21:15,FAIL,Edge/121.0,api.example.com,df_hash_8901,sess_016\n",
        "203.0.113.10,2025-06-15 21:16,FAIL,Safari/17.0,login.example.com,df_hash_8901,sess_017\n",
        "203.0.113.1,2025-06-15 21:17,FAIL,Edge/121.0,api.example.com,df_hash_1234,sess_018\n",
        "203.0.113.3,2025-06-15 21:18,FAIL,Chrome/120.0,login.example.com,df_hash_9012,sess_019\n",
        "203.0.113.6,2025-06-15 21:19,FAIL,Firefox/115.0,api.example.com,df_hash_2345,sess_020\n",
        "\"\"\"\n",
        "df = pd.read_csv(StringIO(data), parse_dates=[\"Timestamp\"])\n",
        "\n",
        "# === Group login attempts by IP and count ===\n",
        "attempts_by_ip = df['IP'].value_counts().reset_index()\n",
        "attempts_by_ip.columns = ['IP', 'Login Attempts']\n",
        "\n",
        "# Filter: IPs with ‚â§ 3 login attempts\n",
        "low_volume = attempts_by_ip[attempts_by_ip['Login Attempts'] <= 3]\n",
        "\n",
        "# === Print Results ===\n",
        "print(\"=== IPs with Low Login Volume (‚â§ 3 attempts) ===\")\n",
        "print(low_volume.to_string(index=False))\n",
        "\n",
        "# === Embedded Answer ===\n",
        "# ‚úÖ Best Answer:\n",
        "# B. Static IP grouping\n",
        "#\n",
        "# üí° Why?\n",
        "# IP rotation causes logins to be distributed across many IPs.\n",
        "# Traditional descriptive analytics assumes repeated failures per IP means suspicious activity.\n",
        "# But each IP here has very few attempts, avoiding detection.\n",
        "# So, static grouping by IP becomes ineffective in detecting distributed botnets.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1-FiDAhVIxc",
        "outputId": "91dbb788-2357-4ba1-b82e-a754025f03ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== IPs with Low Login Volume (‚â§ 3 attempts) ===\n",
            "          IP  Login Attempts\n",
            " 203.0.113.1               3\n",
            " 203.0.113.3               3\n",
            " 203.0.113.6               3\n",
            "203.0.113.10               2\n",
            " 203.0.113.4               2\n",
            " 203.0.113.7               2\n",
            " 203.0.113.9               2\n",
            " 203.0.113.2               1\n",
            " 203.0.113.5               1\n",
            " 203.0.113.8               1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*************************************************************************************************************"
      ],
      "metadata": {
        "id": "i8hDqlvMXeRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2\n",
        "(case 1) What is the most significant limitation of Cross-IP Correlation (Pearson >0.7) when detecting botnet synchronization in a distributed credential stuffing attack?\n",
        "\t\tIt fails to analyze User Agent string diversity\n",
        "\t\t\"It requires centralized log storage, violating GDPR\"\n",
        "\t\tIt misses loosely coordinated attacks with correlations below 0.7\n",
        "\t\tIt is ineffective against static IP-based attacks\n",
        "\t\tIt cannot detect DGA domains used by botnets\n",
        "\t\t\"It scales poorly for large botnets (>10,000 nodes)\"\n"
      ],
      "metadata": {
        "id": "RcLZzZcTVVjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Simulated Pearson correlation values between IP pairs in a botnet\n",
        "np.random.seed(42)\n",
        "correlations = np.random.uniform(0, 1, 1000)  # 1000 pairwise correlations\n",
        "\n",
        "# Threshold for detection\n",
        "threshold = 0.7\n",
        "\n",
        "# Detected coordinated IP pairs (above threshold)\n",
        "detected = correlations[correlations > threshold]\n",
        "missed = correlations[correlations <= threshold]\n",
        "\n",
        "print(f\"Total IP pairs analyzed: {len(correlations)}\")\n",
        "print(f\"Detected coordinated pairs (correlation > {threshold}): {len(detected)}\")\n",
        "print(f\"Missed loosely coordinated pairs (correlation ‚â§ {threshold}): {len(missed)}\")\n",
        "\n",
        "# Embedded Answer Summary\n",
        "'''\n",
        "‚úÖ Best Answer:\n",
        "C. It misses loosely coordinated attacks with correlations below 0.7\n",
        "\n",
        "üí° Why?\n",
        "Cross-IP correlation methods rely on strong similarity signals to detect coordination.\n",
        "Loosely synchronized attacks fall below high correlation thresholds and thus evade detection.\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "sOTz0MrOVbvj",
        "outputId": "308d7813-1e25-4975-9abe-eb8bbb01ca78"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total IP pairs analyzed: 1000\n",
            "Detected coordinated pairs (correlation > 0.7): 288\n",
            "Missed loosely coordinated pairs (correlation ‚â§ 0.7): 712\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n‚úÖ Best Answer:\\nC. It misses loosely coordinated attacks with correlations below 0.7\\n\\nüí° Why?\\nCross-IP correlation methods rely on strong similarity signals to detect coordination.\\nLoosely synchronized attacks fall below high correlation thresholds and thus evade detection.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "************************************************************************************************************************************"
      ],
      "metadata": {
        "id": "u0639eKRXi4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Best Answer:\n",
        "C. It misses loosely coordinated attacks with correlations below 0.7\n",
        "\n",
        "üí° Why?\n",
        "Cross-IP correlation techniques (e.g., Pearson correlation > 0.7) detect strongly synchronized behavior across IPs, such as botnets executing actions nearly simultaneously or in highly similar patterns.\n",
        "\n",
        "Limitation: Many advanced botnets use loosely coordinated tactics‚Äîthey deliberately introduce variability in timing, IP rotation, and user agents to avoid strong correlation signals.\n",
        "\n",
        "If the correlation threshold is too high (e.g., >0.7), these subtle, distributed attacks evade detection.\n",
        "\n",
        "Other options are true concerns but less significant or not the primary limitation in this case:\n",
        "\n",
        "User agent diversity is not directly related to correlation thresholds.\n",
        "\n",
        "GDPR concerns are real but mitigated by federated analytics.\n",
        "\n",
        "Static IP-based attacks are a different problem.\n",
        "\n",
        "DGA detection is a separate analytic dimension.\n",
        "\n",
        "Scalability issues exist but can be addressed via optimizations."
      ],
      "metadata": {
        "id": "7ZulYWwUV4UV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*****************************************************************************************************************************"
      ],
      "metadata": {
        "id": "SXz1vbzUXzTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7\n",
        "(case 1) What unique advantage do Federated GNNs offer over Basic Rate Limiting in defending against credential stuffing attacks in a GDPR-compliant environment?\n",
        "\t\tThey model behavioral patterns across distributed data without pooling sensitive logs\n",
        "\t\tThey block excessive login attempts with dynamic thresholds\n",
        "\t\tThey eliminate false positives in failure rate analysis\n",
        "\t\tThey automatically detect all DGA domains\n",
        "\t\t\"They scale linearly for botnets exceeding 10,000 nodes\"\n",
        "\t\tThey bypass the need for CAPTCHA challenges\n"
      ],
      "metadata": {
        "id": "QFeLAkQLV7l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated descriptive printout to emphasize advantage\n",
        "\n",
        "def federated_gnn_advantage():\n",
        "    \"\"\"\n",
        "    Simulates the core advantage of Federated GNNs in GDPR-compliant credential stuffing defense.\n",
        "    \"\"\"\n",
        "    advantage = (\n",
        "        \"Federated GNNs model behavioral patterns across distributed data without \"\n",
        "        \"pooling sensitive logs, enabling advanced detection while preserving privacy.\"\n",
        "    )\n",
        "    print(\"=== Federated GNN Unique Advantage ===\")\n",
        "    print(advantage)\n",
        "\n",
        "    # Embedded Answer Summary\n",
        "    \"\"\"\n",
        "    ‚úÖ Best Answer:\n",
        "    A. They model behavioral patterns across distributed data without pooling sensitive logs\n",
        "\n",
        "    üí° Why?\n",
        "    Federated GNNs respect data privacy regulations by analyzing distributed data in place.\n",
        "    This enables detection of sophisticated botnet behaviors that basic rate limiting misses.\n",
        "    \"\"\"\n",
        "\n",
        "federated_gnn_advantage()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIBi0_V3WGyB",
        "outputId": "81abf057-69d5-4819-9c07-492b4dad3f6d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Federated GNN Unique Advantage ===\n",
            "Federated GNNs model behavioral patterns across distributed data without pooling sensitive logs, enabling advanced detection while preserving privacy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**************************************************************************************************"
      ],
      "metadata": {
        "id": "iRjzGqEeWVWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Best Answer:\n",
        "A. They model behavioral patterns across distributed data without pooling sensitive logs\n",
        "\n",
        "üí° Why?\n",
        "Federated Graph Neural Networks (GNNs) can analyze complex relationships and behaviors across distributed datasets held locally by multiple parties.\n",
        "\n",
        "This allows them to detect coordinated attacks and subtle patterns (like botnet synchronization, user-device relationships) without centralizing sensitive data, thus respecting GDPR and privacy regulations.\n",
        "\n",
        "Basic rate limiting only blocks excessive attempts per IP or user, lacking insight into complex coordinated attack patterns.\n",
        "\n",
        "Other options:\n",
        "\n",
        "Dynamic thresholds are a feature of rate limiting but not unique to Federated GNNs.\n",
        "\n",
        "False positives and DGA detection are separate issues.\n",
        "\n",
        "Linear scalability and CAPTCHA bypass are not inherent advantages here."
      ],
      "metadata": {
        "id": "_kZ5x1jBWYa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*******************************************************************************************************"
      ],
      "metadata": {
        "id": "rLZNgUBaWoiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8\n",
        "(case 1) Under what condition would User Agent Entropy (Shannon entropy) fail to distinguish automated credential stuffing traffic from legitimate user activity?\n",
        "\t\tWhen User Agent strings exceed 10 unique types per IP\n",
        "\t\tWhen DNS queries include DGA domains\n",
        "\t\tWhen login failure rates are below 50%\n",
        "\t\tWhen adaptive delays exceed 120 seconds\n",
        "\t\t\"When botnets use fewer than 1,000 compromised devices\"\n",
        "\t\tWhen attackers mimic legitimate browser distribution patterns\n"
      ],
      "metadata": {
        "id": "46KlJ59dWsJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import log2\n",
        "\n",
        "# Sample User Agent strings distribution (realistic browser distribution)\n",
        "legit_user_agents = ['Chrome'] * 60 + ['Firefox'] * 25 + ['Safari'] * 15\n",
        "\n",
        "# Simulated User Agent strings from botnet mimicking legit distribution\n",
        "botnet_user_agents = ['Chrome'] * 60 + ['Firefox'] * 25 + ['Safari'] * 15\n",
        "\n",
        "def shannon_entropy(items):\n",
        "    counts = pd.Series(items).value_counts()\n",
        "    probabilities = counts / counts.sum()\n",
        "    return -sum(p * log2(p) for p in probabilities)\n",
        "\n",
        "# Calculate entropy for legitimate users and mimicking botnet\n",
        "entropy_legit = shannon_entropy(legit_user_agents)\n",
        "entropy_botnet = shannon_entropy(botnet_user_agents)\n",
        "\n",
        "print(f\"User Agent Entropy - Legitimate Users: {entropy_legit:.3f}\")\n",
        "print(f\"User Agent Entropy - Botnet Mimicking Legitimate: {entropy_botnet:.3f}\")\n",
        "\n",
        "# Embedded Answer Summary\n",
        "'''\n",
        "‚úÖ Best Answer:\n",
        "F. When attackers mimic legitimate browser distribution patterns\n",
        "\n",
        "üí° Why?\n",
        "User Agent entropy fails to distinguish traffic if attackers mimic realistic User Agent distributions,\n",
        "making automated attacks appear as normal user diversity.\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "31tabttGWrxz",
        "outputId": "9ad9eabf-4b80-4dda-a6ad-fa0ac2ef5ad4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Agent Entropy - Legitimate Users: 1.353\n",
            "User Agent Entropy - Botnet Mimicking Legitimate: 1.353\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n‚úÖ Best Answer:\\nF. When attackers mimic legitimate browser distribution patterns\\n\\nüí° Why?\\nUser Agent entropy fails to distinguish traffic if attackers mimic realistic User Agent distributions,\\nmaking automated attacks appear as normal user diversity.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*******************************************************************************************************"
      ],
      "metadata": {
        "id": "0Z6IUXYoW2Ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10\n",
        "(case 1) How does the Hurst Exponent (H>0.7) enhance detection of bot-like behavior in credential stuffing attacks with adaptive delays?\n",
        "\t\tIt flags high failure rates across IP clusters\n",
        "\t\tIt measures diversity in User Agent strings\n",
        "\t\tIt detects unique DNS query patterns\n",
        "\t\tIt correlates CAPTCHA solve times\n",
        "\t\tIt identifies persistent timing patterns despite randomized delays\n",
        "\t\tIt maps attack graphs via community detection\n"
      ],
      "metadata": {
        "id": "1PvsuBNqW5l6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "******************************************************************************************************"
      ],
      "metadata": {
        "id": "MDVeM_MUW_eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def hurst_exponent(time_series):\n",
        "    \"\"\"\n",
        "    Estimate the Hurst Exponent of a time series.\n",
        "    H > 0.7 indicates persistence (long memory).\n",
        "    \"\"\"\n",
        "    N = len(time_series)\n",
        "    T = np.arange(1, N + 1)\n",
        "    Y = np.cumsum(time_series - np.mean(time_series))\n",
        "    R = np.max(Y) - np.min(Y)\n",
        "    S = np.std(time_series)\n",
        "    if S == 0:\n",
        "        return 0\n",
        "    return np.log(R / S) / np.log(N)\n",
        "\n",
        "# Simulated inter-arrival times (seconds) with adaptive delays but persistent pattern\n",
        "np.random.seed(0)\n",
        "base_delays = np.linspace(30, 120, 50)  # increasing delays\n",
        "noise = np.random.uniform(-5, 5, 50)   # some noise\n",
        "adaptive_delays = base_delays + noise\n",
        "\n",
        "H = hurst_exponent(adaptive_delays)\n",
        "\n",
        "print(f\"Hurst Exponent (H) of adaptive delays: {H:.3f}\")\n",
        "\n",
        "# Embedded answer summary\n",
        "'''\n",
        "‚úÖ Best Answer:\n",
        "E. It identifies persistent timing patterns despite randomized delays\n",
        "\n",
        "üí° Why?\n",
        "The Hurst Exponent detects long-term persistence in timing data, revealing bot-like behavior even when delays are randomized.\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "dnf3ANm2XCJq",
        "outputId": "ab471020-73a9-40b6-fc68-fffee7c7e1fb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hurst Exponent (H) of adaptive delays: 0.785\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n‚úÖ Best Answer:  \\nE. It identifies persistent timing patterns despite randomized delays\\n\\nüí° Why?  \\nThe Hurst Exponent detects long-term persistence in timing data, revealing bot-like behavior even when delays are randomized.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***********************************************************************************************************************"
      ],
      "metadata": {
        "id": "TC2msityXIX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Best Answer:\n",
        "E. It identifies persistent timing patterns despite randomized delays\n",
        "\n",
        "üí° Why?\n",
        "The Hurst Exponent (H) measures the long-term memory or persistence in time series data.\n",
        "\n",
        "When H > 0.7, it indicates persistent behavior ‚Äî future values tend to follow past trends rather than random noise.\n",
        "\n",
        "In credential stuffing attacks with adaptive delays (30‚Äì120 seconds randomized pauses), attackers try to evade detection by randomizing timing.\n",
        "\n",
        "Despite this, the attacker's behavior often shows persistent timing patterns detectable by Hurst Exponent analysis.\n",
        "\n",
        "Other options relate to different analytics:\n",
        "\n",
        "Failure rates (A) and User Agent diversity (B) are unrelated to Hurst Exponent.\n",
        "\n",
        "DNS queries (C), CAPTCHA times (D), and graph mapping (F) are different techniques.\n",
        "\n"
      ],
      "metadata": {
        "id": "Wl7Gp48QXKys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***********************************************************************************************************"
      ],
      "metadata": {
        "id": "Dq0CXbirXSPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LQaS8BY7Wq8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "******************************************************************************************************************************"
      ],
      "metadata": {
        "id": "9d9e0_7BYOSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Code Setup for Scenario 2: Supply Chain Attack Analytics (Operation ShadowForge)"
      ],
      "metadata": {
        "id": "96QyqqMWYR4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "from collections import Counter\n",
        "from math import log2\n",
        "\n",
        "# Simulated CSV data for the case study\n",
        "csv_data = \"\"\"Build ID,Timestamp,Commit Author,Artifact Hash,Trigger Type,Dependency Name,Dependency Source,Alert Severity,Pod Name\n",
        "BLD001,2025-06-17 15:30:00,dev1@nexlify.com,sha256:abc123,Manual,nexlify-utils,npmjs.com,Low,ci-pod-1\n",
        "BLD001,2025-06-17 15:30:30,dev1@nexlify.com,sha256:abc124,Scheduled,nexlify_utilz,malico.us/npm,Critical,ci-pod-2\n",
        "BLD002,2025-06-17 15:30:10,dev2@nexlify.com,sha256:def456,Manual,express,npmjs.com,Low,ci-pod-3\n",
        "BLD003,2025-06-17 15:30:20,dev3@nexlify.com,sha256:ghi789,Scheduled,nexlify-core,malico.us/npm,Critical,ci-pod-1\n",
        "BLD003,2025-06-17 15:30:40,dev3@nexlify.com,sha256:ghi790,Manual,lodash,npmjs.com,Low,ci-pod-4\n",
        "BLD004,2025-06-17 15:30:05,dev4@nexlify.com,sha256:jkl012,Manual,react,npmjs.com,Low,ci-pod-5\n",
        "BLD004,2025-06-17 15:30:35,dev4@nexlify.com,sha256:jkl013,Scheduled,nexlify-api,malico.us/npm,High,ci-pod-2\n",
        "BLD005,2025-06-17 15:30:15,dev5@nexlify.com,sha256:mno345,Manual,angular,npmjs.com,Low,ci-pod-6\n",
        "BLD006,2025-06-17 15:30:25,dev6@nexlify.com,sha256:pqr678,Scheduled,nexlify-auth,malico.us/npm,Critical,ci-pod-1\n",
        "BLD006,2025-06-17 15:30:45,dev6@nexlify.com,sha256:pqr679,Manual,vue,npmjs.com,Low,ci-pod-7\n",
        "BLD007,2025-06-17 15:30:00,dev7@nexlify.com,sha256:stu901,Manual,webpack,npmjs.com,Low,ci-pod-8\n",
        "BLD007,2025-06-17 15:30:30,dev7@nexlify.com,sha256:stu902,Scheduled,nexlify-sdk,malico.us/npm,High,ci-pod-2\n",
        "BLD008,2025-06-17 15:30:10,dev8@nexlify.com,sha256:vwx234,Manual,typescript,npmjs.com,Low,ci-pod-9\n",
        "BLD009,2025-06-17 15:30:20,dev9@nexlify.com,sha256:yza567,Scheduled,nexlify-logger,malico.us/npm,Critical,ci-pod-1\n",
        "BLD009,2025-06-17 15:30:40,dev9@nexlify.com,sha256:yza568,Manual,jest,npmjs.com,Low,ci-pod-10\n",
        "BLD010,2025-06-17 15:30:05,dev10@nexlify.com,sha256:bcd890,Scheduled,nexlify-monitor,malico.us/npm,Critical,ci-pod-2\n",
        "BLD010,2025-06-17 15:30:35,dev10@nexlify.com,sha256:bcd891,Manual,mocha,npmjs.com,Low,ci-pod-11\n",
        "BLD001,2025-06-17 15:30:50,dev1@nexlify.com,sha256:abc125,Scheduled,nexlify-security,malico.us/npm,High,ci-pod-1\n",
        "BLD003,2025-06-17 15:30:55,dev3@nexlify.com,sha256:ghi791,Manual,axios,npmjs.com,Low,ci-pod-12\n",
        "BLD006,2025-06-17 15:31:00,dev6@nexlify.com,sha256:pqr680,Scheduled,nexlify-analytics,malico.us/npm,Critical,ci-pod-2\n",
        "\"\"\"\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(StringIO(csv_data), parse_dates=[\"Timestamp\"])\n",
        "\n",
        "# ========== 1. Artifact Integrity Score ==========\n",
        "# Simplified: Count unique hashes per build to find suspicious variance\n",
        "artifact_integrity = df.groupby(\"Build ID\")[\"Artifact Hash\"].nunique().rename(\"Artifact Integrity Score\")\n",
        "\n",
        "# ========== 2. Pipeline Entropy (per build) ==========\n",
        "def calculate_entropy(values):\n",
        "    counter = Counter(values)\n",
        "    total = sum(counter.values())\n",
        "    return -sum((count/total) * log2(count/total) for count in counter.values() if count > 0)\n",
        "\n",
        "entropy_scores = (\n",
        "    df.groupby(\"Build ID\")\n",
        "    .apply(lambda group: calculate_entropy(group[\"Commit Author\"]))\n",
        "    .rename(\"Pipeline Entropy\")\n",
        ")\n",
        "\n",
        "# ========== 3. Dependency Diversity (per build) ==========\n",
        "dependency_diversity = df.groupby(\"Build ID\")[\"Dependency Name\"].nunique().rename(\"Dependency Diversity\")\n",
        "\n",
        "# ========== 4. Merge All Diagnostic Indicators ==========\n",
        "diagnostics = pd.concat([artifact_integrity, entropy_scores, dependency_diversity], axis=1).reset_index()\n",
        "\n",
        "# ========== 5. Alert Statistics per Build ==========\n",
        "alert_summary = (\n",
        "    df.groupby([\"Build ID\", \"Alert Severity\"])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Final Diagnostic View\n",
        "final_report = pd.merge(diagnostics, alert_summary, on=\"Build ID\", how=\"left\")\n",
        "\n",
        "# Display\n",
        "print(\"üîç Final Supply Chain Diagnostic Report:\\n\")\n",
        "print(final_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISRxQk9VYT1m",
        "outputId": "085018a2-9dee-4eac-bdd6-8c9500fdeca7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Final Supply Chain Diagnostic Report:\n",
            "\n",
            "  Build ID  Artifact Integrity Score  Pipeline Entropy  Dependency Diversity  \\\n",
            "0   BLD001                         3              -0.0                     3   \n",
            "1   BLD002                         1              -0.0                     1   \n",
            "2   BLD003                         3              -0.0                     3   \n",
            "3   BLD004                         2              -0.0                     2   \n",
            "4   BLD005                         1              -0.0                     1   \n",
            "5   BLD006                         3              -0.0                     3   \n",
            "6   BLD007                         2              -0.0                     2   \n",
            "7   BLD008                         1              -0.0                     1   \n",
            "8   BLD009                         2              -0.0                     2   \n",
            "9   BLD010                         2              -0.0                     2   \n",
            "\n",
            "   Critical  High  Low  \n",
            "0         1     1    1  \n",
            "1         0     0    1  \n",
            "2         1     0    2  \n",
            "3         0     1    1  \n",
            "4         0     0    1  \n",
            "5         2     0    1  \n",
            "6         0     1    1  \n",
            "7         0     0    1  \n",
            "8         1     0    1  \n",
            "9         1     0    1  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-3753160144.py:46: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda group: calculate_entropy(group[\"Commit Author\"]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*****************************************************************************************************************************"
      ],
      "metadata": {
        "id": "ozs2oAU2YiHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ **Key Metrics Computed**\n",
        "Artifact Integrity Score ‚Üí Count of different hashes per Build ID\n",
        "\n",
        "Pipeline Entropy ‚Üí Shannon entropy of authors per Build ID (diversity of activity)\n",
        "\n",
        "Dependency Diversity ‚Üí Unique dependencies per build (detects confusion)\n",
        "\n",
        "üí°** Why this matters:**\n",
        "This script will serve as the core analytical engine to:\n",
        "\n",
        "Detect low-volume tampering,\n",
        "\n",
        "Spot impersonation via log entropy,\n",
        "\n",
        "Flag dependency confusion using string similarity (to be added in later steps),\n",
        "\n",
        "Monitor evasion using statistical irregularities."
      ],
      "metadata": {
        "id": "zVNSmBaKYjX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****************************************************************************************************************************************"
      ],
      "metadata": {
        "id": "ibLL_zpDY3b7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3\n",
        "\"(case 2) What does a high Pipeline Entropy value (e.g., 0.918296 for BLD001) indicate about the likelihood of pipeline spoofing in this supply chain attack?\"\n",
        "\t\t\"It indicates low dependency diversity, typical of legitimate builds\"\n",
        "\t\t\"It flags high artifact integrity, ruling out tampering\"\n",
        "\t\t\"It suggests diverse commit authors, potentially masking forged developer accounts\"\n",
        "\t\tIt detects adversarial noise in pipeline logs\n",
        "\t\tIt confirms secure API token usage\n",
        "\t\tIt correlates with low autoencoder anomaly scores\n"
      ],
      "metadata": {
        "id": "qQYFe7kyY5n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall entropy scores per Build ID computed earlier\n",
        "print(entropy_scores)\n",
        "\n",
        "# Interpretation logic based on entropy values\n",
        "build_id = \"BLD001\"\n",
        "entropy_value = entropy_scores.loc[build_id]\n",
        "\n",
        "print(f\"Pipeline Entropy for {build_id}: {entropy_value:.6f}\")\n",
        "\n",
        "# Explanation and answer choice selection\n",
        "# High entropy = high diversity of commit authors or activities, which can mask forgery or spoofing attempts.\n",
        "\n",
        "# ‚úÖ Best Answer:\n",
        "# C. It suggests diverse commit authors, potentially masking forged developer accounts\n",
        "\n",
        "# üí° Why?\n",
        "# A high Pipeline Entropy value indicates many different commit authors or triggers are involved.\n",
        "# This diversity could be legitimate or malicious; in this case, attackers spoof developer accounts,\n",
        "# increasing entropy to evade detection. Hence, high entropy signals possible pipeline spoofing.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFOuAbaPY_lu",
        "outputId": "148c75b9-8954-4242-f169-e3e87e73e8c3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build ID\n",
            "BLD001   -0.0\n",
            "BLD002   -0.0\n",
            "BLD003   -0.0\n",
            "BLD004   -0.0\n",
            "BLD005   -0.0\n",
            "BLD006   -0.0\n",
            "BLD007   -0.0\n",
            "BLD008   -0.0\n",
            "BLD009   -0.0\n",
            "BLD010   -0.0\n",
            "Name: Pipeline Entropy, dtype: float64\n",
            "Pipeline Entropy for BLD001: -0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "************************************************************************************************************"
      ],
      "metadata": {
        "id": "Op9jTHJdZLBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Best Answer:\n",
        "C. It suggests diverse commit authors, potentially masking forged developer accounts\n",
        "\n",
        "üí° Why?\n",
        "Pipeline Entropy measures the randomness or diversity of pipeline activities (commit authors, triggers).\n",
        "\n",
        "High entropy means many different authors appear, which can be normal or an evasion tactic.\n",
        "\n",
        "Attackers spoof logs to imitate multiple developers, inflating entropy and making detection harder.\n",
        "\n",
        "So, a high entropy value flags pipeline spoofing attempts rather than confirming security or low anomalies."
      ],
      "metadata": {
        "id": "glbPxdtkZNt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*************************************************************************************************************************"
      ],
      "metadata": {
        "id": "mhvjxTH6ZPhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4\n",
        "\"(case 2) How does the use of federated analytics (e.g., PySyft) enhance the effectiveness of diagnostic analytics in this GDPR-compliant environment compared to traditional Hash Whitelisting?\"\n",
        "\t\tIt automatically blocks all tampered artifacts\n",
        "\t\tIt eliminates the need for artifact signing\n",
        "\t\tIt detects all dependency confusion attacks\n",
        "\t\tIt provides real-time adversarial noise detection\n",
        "\t\tIt ensures zero false positives in anomaly detection\n",
        "\t\tIt enables distributed analysis of pipeline logs without pooling sensitive data\n"
      ],
      "metadata": {
        "id": "XNFFmpjjZSYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Correct: no use of walrus operator inside dictionary\n",
        "\n",
        "federated_analytics_advantages = {\n",
        "    \"Automatic blocking\": False,\n",
        "    \"Eliminate artifact signing\": False,\n",
        "    \"Detect all dependency confusion\": False,\n",
        "    \"Real-time adversarial noise detection\": True,  # partially true, still marked as True for discussion\n",
        "    \"Zero false positives\": False,\n",
        "    \"Distributed sensitive data analysis\": True,\n",
        "}\n",
        "\n",
        "print(\"Federated Analytics advantages in GDPR-compliant environment:\\n\")\n",
        "for feature, enabled in federated_analytics_advantages.items():\n",
        "    print(f\"- {feature}: {'Yes' if enabled else 'No'}\")\n",
        "\n",
        "# ‚úÖ Best Answer:\n",
        "# It enables distributed analysis of pipeline logs without pooling sensitive data\n",
        "\n",
        "# üí° Why?\n",
        "# Federated analytics (using tools like PySyft) enables teams to analyze logs locally across pods/clouds\n",
        "# without transferring sensitive data, preserving GDPR compliance. Traditional methods like hash\n",
        "# whitelisting require centralization, which is riskier and potentially non-compliant.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY4ba61-aMpl",
        "outputId": "dc54b870-94d8-48f8-ae23-1498703ce886"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Federated Analytics advantages in GDPR-compliant environment:\n",
            "\n",
            "- Automatic blocking: No\n",
            "- Eliminate artifact signing: No\n",
            "- Detect all dependency confusion: No\n",
            "- Real-time adversarial noise detection: Yes\n",
            "- Zero false positives: No\n",
            "- Distributed sensitive data analysis: Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*************************************************************************************************************************"
      ],
      "metadata": {
        "id": "myv3ZB8uaXQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ **Best Answer:**\n",
        "It enables distributed analysis of pipeline logs without pooling sensitive data\n",
        "\n",
        "üí° **Why?**\n",
        "Federated analytics allows collaborative analytics while keeping data local, respecting GDPR.\n",
        "\n",
        "It avoids the privacy and compliance risks of centralized data pooling.\n",
        "\n",
        "Traditional hash whitelisting relies on a central repository of hashes, increasing risk of data leaks.\n",
        "\n",
        "Distributed analytics enables timely, privacy-preserving detection of supply chain attacks."
      ],
      "metadata": {
        "id": "pBjwVJGVaZ_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**************************************************************************************************************"
      ],
      "metadata": {
        "id": "JLLPSIOvac-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5\n",
        "(case 2) Why might Dependency Diversity fail to detect dependency confusion if attackers use a single malicious dependency per build?\n",
        "\t\tIt cannot process Shannon entropy for dependency names\n",
        "\t\tIt relies on multiple unique dependencies to flag suspicious sources\n",
        "\t\t\"It is designed to detect pipeline spoofing, not dependency issues\"\n",
        "\t\t\"It requires centralized log storage, violating GDPR\"\n",
        "\t\tIt is ineffective against low-volume builds\n",
        "\t\tIt fails to analyze artifact hash deviations\n"
      ],
      "metadata": {
        "id": "_gOGd2CHafGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall Dependency Diversity per build\n",
        "print(dependency_diversity)\n",
        "\n",
        "# Explanation about why Dependency Diversity might fail for single malicious dependency\n",
        "# We look at builds with low dependency diversity but known malicious activity\n",
        "\n",
        "single_malicious_dependency_builds = df.groupby(\"Build ID\").filter(\n",
        "    lambda x: (x[\"Dependency Source\"].str.contains(\"malico\", case=False).sum() == 1)\n",
        ")\n",
        "\n",
        "print(\"Builds with exactly one malicious dependency (possible detection blindspot):\")\n",
        "print(single_malicious_dependency_builds[[\"Build ID\", \"Dependency Name\", \"Dependency Source\"]])\n",
        "\n",
        "# ‚úÖ Best Answer:\n",
        "# B. It relies on multiple unique dependencies to flag suspicious sources\n",
        "\n",
        "# üí° Why?\n",
        "# Dependency Diversity measures how many unique external dependencies a build has.\n",
        "# If attackers inject only one malicious dependency per build (low volume),\n",
        "# the diversity metric may remain low, failing to flag the build as suspicious.\n",
        "# Thus, it depends on seeing multiple unique malicious dependencies to raise alarms.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajn52ctOajoW",
        "outputId": "66e6ff2b-694e-4913-a09d-2660bab4e3ab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build ID\n",
            "BLD001    3\n",
            "BLD002    1\n",
            "BLD003    3\n",
            "BLD004    2\n",
            "BLD005    1\n",
            "BLD006    3\n",
            "BLD007    2\n",
            "BLD008    1\n",
            "BLD009    2\n",
            "BLD010    2\n",
            "Name: Dependency Diversity, dtype: int64\n",
            "Builds with exactly one malicious dependency (possible detection blindspot):\n",
            "   Build ID  Dependency Name Dependency Source\n",
            "3    BLD003     nexlify-core     malico.us/npm\n",
            "4    BLD003           lodash         npmjs.com\n",
            "5    BLD004            react         npmjs.com\n",
            "6    BLD004      nexlify-api     malico.us/npm\n",
            "10   BLD007          webpack         npmjs.com\n",
            "11   BLD007      nexlify-sdk     malico.us/npm\n",
            "13   BLD009   nexlify-logger     malico.us/npm\n",
            "14   BLD009             jest         npmjs.com\n",
            "15   BLD010  nexlify-monitor     malico.us/npm\n",
            "16   BLD010            mocha         npmjs.com\n",
            "18   BLD003            axios         npmjs.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*******************************************************************************************************"
      ],
      "metadata": {
        "id": "S97x6CX1apkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ **Best Answer:**\n",
        "B. It relies on multiple unique dependencies to flag suspicious sources\n",
        "\n",
        "üí°** Why?**\n",
        "Dependency Diversity counts unique dependencies per build.\n",
        "\n",
        "Single malicious dependency injections don‚Äôt increase diversity significantly.\n",
        "\n",
        "Therefore, low-volume or single malicious dependencies can evade detection using this metric.\n",
        "\n",
        "Other methods (e.g., anomaly detection or hash deviation) must complement it for robust detection.\n",
        "\n"
      ],
      "metadata": {
        "id": "YCZFLv2VasaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*****************************************************************************************************************"
      ],
      "metadata": {
        "id": "ipnXBRk7ayjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6\n",
        "(case 2) Which analytic is most directly undermined by attackers injecting subtle malicious code that mimics legitimate commits in the CI/CD pipeline?\n",
        "\t\tPipeline Entropy\n",
        "\t\tDependency Diversity\n",
        "\t\tAutoencoder Anomaly Detection\n",
        "\t\tArtifact Integrity Score\n",
        "\t\tHash Whitelisting\n",
        "\t\tStatic Log Filtering\n"
      ],
      "metadata": {
        "id": "tH2t2KTDa0pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall key metrics: Artifact Integrity Score measures hash deviations per build\n",
        "print(artifact_integrity)\n",
        "\n",
        "# Explanation:\n",
        "# Subtle malicious code that mimics legitimate commits likely does NOT change the hash drastically,\n",
        "# making hash-based checks or artifact integrity scoring less effective if the tampering is subtle.\n",
        "\n",
        "# However, subtle tampering directly targets the integrity of artifacts,\n",
        "# so Artifact Integrity Score is the analytic designed to detect any hash deviations or tampering.\n",
        "\n",
        "# ‚úÖ Best Answer:\n",
        "# D. Artifact Integrity Score\n",
        "\n",
        "# üí° Why?\n",
        "# Artifact Integrity Score focuses on detecting any tampering or deviation from baseline artifact hashes.\n",
        "# Even if the malicious code is subtle, this analytic attempts to flag discrepancies in the artifact signatures.\n",
        "# Other analytics like Pipeline Entropy or Dependency Diversity are more indirect or behavioral.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW0pNMkXa6yy",
        "outputId": "622551c8-5c22-4213-f6c3-df2742d6d709"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build ID\n",
            "BLD001    3\n",
            "BLD002    1\n",
            "BLD003    3\n",
            "BLD004    2\n",
            "BLD005    1\n",
            "BLD006    3\n",
            "BLD007    2\n",
            "BLD008    1\n",
            "BLD009    2\n",
            "BLD010    2\n",
            "Name: Artifact Integrity Score, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*************************************************************************************************************************"
      ],
      "metadata": {
        "id": "IFNgwV04a_8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ** Best Answer:**\n",
        "D. Artifact Integrity Score\n",
        "\n",
        "üí°** Why?**\n",
        "Artifact Integrity Score detects tampering by measuring hash deviations.\n",
        "\n",
        "Even subtle malicious code changes can alter artifact hashes.\n",
        "\n",
        "Other analytics focus on behavioral diversity or anomaly patterns but not direct artifact tampering.\n",
        "\n",
        "Therefore, Artifact Integrity Score is most directly undermined or tested by such attacks.\n",
        "\n"
      ],
      "metadata": {
        "id": "oEbccmfGbC9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**********************************************************************************************************************************"
      ],
      "metadata": {
        "id": "MOrt1D3AbK_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9\n",
        "(case 2) What is the primary limitation of Autoencoder Anomaly Detection in identifying malicious builds when attackers inject adversarial noise?\n",
        "\t\tIt cannot detect dependency confusion in npm packages\n",
        "\t\tIt requires high pipeline entropy to function effectively\n",
        "\t\tIt is limited to analyzing artifact integrity scores\n",
        "\t\tIt scales poorly for large Kubernetes environments\n",
        "\t\t\"Adversarial noise poisons the training data, reducing anomaly score accuracy\"\n",
        "\t\tIt misses low-volume malicious builds"
      ],
      "metadata": {
        "id": "3Xhj8H12bNRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conceptual limitations of Autoencoder Anomaly Detection in presence of adversarial noise\n",
        "\n",
        "autoencoder_limitations = {\n",
        "    \"Detects dependency confusion\": False,\n",
        "    \"Requires high entropy\": False,\n",
        "    \"Limited to artifact scores\": False,\n",
        "    \"Scales poorly in Kubernetes\": False,\n",
        "    \"Poisoning training data\": True,  # Correct answer\n",
        "    \"Misses low-volume builds\": True  # Also a valid limitation, but not the primary one in this context\n",
        "}\n",
        "\n",
        "print(\"Autoencoder Anomaly Detection Limitations:\\n\")\n",
        "for description, is_limitation in autoencoder_limitations.items():\n",
        "    print(f\"- {description}: {'Yes' if is_limitation else 'No'}\")\n",
        "\n",
        "# ‚úÖ Best Answer:\n",
        "# \"Adversarial noise poisons the training data, reducing anomaly score accuracy\"\n",
        "\n",
        "# üí° Why?\n",
        "# Autoencoders are trained on what they learn as \"normal\" patterns.\n",
        "# Adversarial noise (benign-looking but attacker-controlled data) corrupts this baseline,\n",
        "# causing the model to miss actual anomalies. This weakens its accuracy and reliability,\n",
        "# making it blind to true malicious builds.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eCXoQXEbvDY",
        "outputId": "cc663a2b-b85a-4f78-819f-1612929d3f61"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencoder Anomaly Detection Limitations:\n",
            "\n",
            "- Detects dependency confusion: No\n",
            "- Requires high entropy: No\n",
            "- Limited to artifact scores: No\n",
            "- Scales poorly in Kubernetes: No\n",
            "- Poisoning training data: Yes\n",
            "- Misses low-volume builds: Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "******************************************************************************************************************************************"
      ],
      "metadata": {
        "id": "66RZMNbsbyLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ **Best Answer:**\n",
        "\"Adversarial noise poisons the training data, reducing anomaly score accuracy\"\n",
        "\n",
        "üí° **Why?**\n",
        "Autoencoders rely on clean training data to learn the \"normal\" pipeline behavior.\n",
        "\n",
        "When attackers inject adversarial noise, the model learns incorrect baselines.\n",
        "\n",
        "This leads to false negatives, where real anomalies are no longer \"anomalous\" to the model."
      ],
      "metadata": {
        "id": "QZQFyf_qb0lM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "********************************************************************************************************"
      ],
      "metadata": {
        "id": "3oFjx6xub8DV"
      }
    }
  ]
}